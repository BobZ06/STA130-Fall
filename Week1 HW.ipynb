{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42b8fdf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PassengerId      0\n",
       "Survived         0\n",
       "Pclass           0\n",
       "Name             0\n",
       "Sex              0\n",
       "Age            177\n",
       "SibSp            0\n",
       "Parch            0\n",
       "Ticket           0\n",
       "Fare             0\n",
       "Cabin          687\n",
       "Embarked         2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the Titanic dataset\n",
    "url = \"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\"\n",
    "df = pd.read_csv(url)\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb5ad73a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp',\n",
      "       'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked'],\n",
      "      dtype='object')\n",
      "(891, 12)\n"
     ]
    }
   ],
   "source": [
    "# Get the columns and the number of rows and columns in the dataset\n",
    "columns = df.columns\n",
    "data_shape = df.shape\n",
    "\n",
    "print(columns) \n",
    "print(data_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d473736",
   "metadata": {},
   "source": [
    "***Question 2.2***\n",
    "\n",
    "Observations are represented by rows in the dataset. Each observation represents a unit of data that was measured and recorded. In my dataset, each observation corresponds to a passenger on the titanic.\n",
    "\n",
    "Variables are represented by columns in the dataset. They are descriptions of the characteristics of the observations. In my dataset, name is one of these variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f10ca9a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       PassengerId    Survived      Pclass         Age       SibSp  \\\n",
      "count   891.000000  891.000000  891.000000  714.000000  891.000000   \n",
      "mean    446.000000    0.383838    2.308642   29.699118    0.523008   \n",
      "std     257.353842    0.486592    0.836071   14.526497    1.102743   \n",
      "min       1.000000    0.000000    1.000000    0.420000    0.000000   \n",
      "25%     223.500000    0.000000    2.000000   20.125000    0.000000   \n",
      "50%     446.000000    0.000000    3.000000   28.000000    0.000000   \n",
      "75%     668.500000    1.000000    3.000000   38.000000    1.000000   \n",
      "max     891.000000    1.000000    3.000000   80.000000    8.000000   \n",
      "\n",
      "            Parch        Fare  \n",
      "count  891.000000  891.000000  \n",
      "mean     0.381594   32.204208  \n",
      "std      0.806057   49.693429  \n",
      "min      0.000000    0.000000  \n",
      "25%      0.000000    7.910400  \n",
      "50%      0.000000   14.454200  \n",
      "75%      0.000000   31.000000  \n",
      "max      6.000000  512.329200  \n"
     ]
    }
   ],
   "source": [
    "# Summary statistics for numerical columns\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e477724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Survived\n",
      "0    549\n",
      "1    342\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Frequency count for the 'Survived' column\n",
    "print(df['Survived'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd1ac6a",
   "metadata": {},
   "source": [
    "***Question 4***\n",
    "\n",
    "a)**The number of columns it analyzes**: df.shape analyzes the entire dataset, so it will include all columns. df.describe() will only analyze the numeric columns in the dataset.\n",
    "\n",
    "b)**the values it reports in the \"count\" column**: df.shape counts all rows in the dataset. df.describe() only counts non-missing values for each column, so for columns with missing values, the count will be lower than the total number of rows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5b67eb",
   "metadata": {},
   "source": [
    "***Question 5***\n",
    "\n",
    "**Attribute**, such as df.shape, is the static property that store information about the object. Therefore they can be accessed directly and do not end with (), as they do not involve any computation.\n",
    "\n",
    "**Method**, such as df.describe(), is the function attached to an object that performs computations on the object and so must be ended with ()."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca9ef5c",
   "metadata": {},
   "source": [
    "***Question 6***\n",
    "\n",
    "**count**:It will count the non-missing values for each column.\n",
    "\n",
    "**mean**:It will calculate the average value of the data in each column.\n",
    "\n",
    "**std**:Standard deviation is the spread or dispersion of the data around mean.\n",
    "\n",
    "**min**:The smallest value in the column.\n",
    "\n",
    "**25%**:The value below which 25% of the data falls (also known as the first quartile or Q1).\n",
    "\n",
    "**50%**:The median or middle value of the data, also known as the second quartile (Q2). Itâ€™s the value below which 50% of the data falls.\n",
    "\n",
    "**75%**:The value below which 75% of the data falls (also known as the third quartile or Q3).\n",
    "\n",
    "**max**:The largest value in the column."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61dad2d",
   "metadata": {},
   "source": [
    "***Question 7.1***\n",
    "\n",
    "The example is the DataFrame with missing values. When I only want to remove rows that contain missing values. df.dropna() is peferred over using del df['col']."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "70af2497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     A    B    C\n",
      "2  3.0  6.0  9.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = {\n",
    "    'A': [1, 2, 3, None],\n",
    "    'B': [4, None, 6, 7],\n",
    "    'C': [None, 8, 9, 10]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Use df.dropna() to remove rows with any NaN values\n",
    "cleaned_df = df.dropna()\n",
    "print(cleaned_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6dcdc6",
   "metadata": {},
   "source": [
    "*In this example, I use* df.dropna() *to remove the rows containing NaN values. The columns with valid data (A, B, C) are retained. If I use* del df['A'], *it will remove the entire column A. Compared to* del df['col'], df.dropna() *offers more flexibility.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1732dfb6",
   "metadata": {},
   "source": [
    "***Question 7.2***\n",
    "\n",
    "The example is the DataFrame where the entire column contains missing values. When I want to remove the entire column, the del df['col'] is more effective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "729c6c73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   A  C\n",
      "0  1  5\n",
      "1  2  6\n",
      "2  3  7\n",
      "3  4  8\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = {\n",
    "    'A': [1, 2, 3, 4],\n",
    "    'B': [None, None, None, None],  # Entire column is NaN\n",
    "    'C': [5, 6, 7, 8]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Use del to remove the irrelevant or fully NaN column 'B'\n",
    "del df['B']\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2fc240",
   "metadata": {},
   "source": [
    "*In this example, I use* del df['B'] *to remove the entire row that contains the missing value. In that case, if I use* df.dropna(), *the code will be* df.dropna(axis=1). *This is more complex compared to* del df['B']. *Therefore, using* del df['B'] *will be more direct.* "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d34ba7",
   "metadata": {},
   "source": [
    "***Question 7.3***\n",
    "\n",
    "If the dataset has the columns that contain only missing value, removing them at first by using del df['col'] will be make the progress more effective. After that, df.dropna() can only process the remaining columns. Removing unnecessary columns can also ensure that df.dropna() works the relevant columns, reducing the probability of making errors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df0ba17",
   "metadata": {},
   "source": [
    "***Question 7.4***\n",
    "\n",
    "The example is the DataFrame contains more missing values. del df['col'] will be used before df.dropna() in that case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0cb420b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     A    C\n",
      "0  1.0  4.0\n",
      "2  3.0  6.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = {\n",
    "    'A': [1, 2, 3, None],\n",
    "    'B': [None, None, None, None],  # Entire column is NaN\n",
    "    'C': [4, None, 6, 7]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Step 1: Remove irrelevant column with only NaNs\n",
    "del df['B']\n",
    "\n",
    "# Step 2: Drop rows with any NaN values in the remaining columns\n",
    "cleaned_df = df.dropna()\n",
    "print(cleaned_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99db00e",
   "metadata": {},
   "source": [
    "*In that case, since column B contains only missing value, so we have to use* del df['B'] *at first to remove the entire column, so that* df.dropna() *can only work on those column A and C, which has the non-missing value. Therefore, the dataset without missing value can be generated. This process ensures that* df.dropna() *focus on the meaningful data only and improve the efficiency.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e9c7c4",
   "metadata": {},
   "source": [
    "***Question 8.1***\n",
    "\n",
    "The expression df.groupby(\"col1\")[\"col2\"].describe() performs the following steps:\n",
    "\n",
    "1) \n",
    "df.groupby(\"col1\"): This groups the dataset df by the unique values in col1. The data is divided into subsets where each subset corresponds to one of the unique values in col1.\n",
    "\n",
    "2) \n",
    "[\"col2\"]: This selects only the column col2 from each of those subsets.\n",
    "\n",
    "3) \n",
    ".describe(): This applies the describe() method to each of those subsets of col2. The describe() method generates descriptive statistics. It gives count, unique, top (most frequent value), and frequency.\n",
    "\n",
    "The example includes the new dataset about characters from animal crossings, and it will show how the df.groupby(\"col1\")[\"col2\"].describe() performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d94efc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          count unique     top freq\n",
      "species                            \n",
      "alligator     7      5    lazy    2\n",
      "anteater      7      6   peppy    2\n",
      "bear         15      7  cranky    5\n",
      "bird         13      7    jock    4\n",
      "bull          6      3  cranky    3\n",
      "cat          23      8  snooty    5\n",
      "chicken       9      7  snooty    2\n",
      "cow           4      3  snooty    2\n",
      "cub          16      7    lazy    4\n",
      "deer         10      7    lazy    2\n",
      "dog          16      8    lazy    6\n",
      "duck         17      6    lazy    4\n",
      "eagle         9      5  cranky    4\n",
      "elephant     11      5    lazy    4\n",
      "frog         18      8    jock    5\n",
      "goat          8      7  normal    2\n",
      "gorilla       9      6  cranky    3\n",
      "hamster       8      7    smug    2\n",
      "hippo         7      6  cranky    2\n",
      "horse        15      8    lazy    3\n",
      "kangaroo      8      4  normal    3\n",
      "koala         9      7  normal    3\n",
      "lion          7      4    jock    3\n",
      "monkey        8      7    lazy    2\n",
      "mouse        15      7   peppy    4\n",
      "octopus       3      3  normal    1\n",
      "ostrich      10      7  snooty    3\n",
      "penguin      13      8    lazy    4\n",
      "pig          15      8    jock    3\n",
      "rabbit       20      8   peppy    8\n",
      "rhino         6      5  normal    2\n",
      "sheep        13      6  snooty    4\n",
      "squirrel     18      8  normal    5\n",
      "tiger         7      4    jock    3\n",
      "wolf         11      5  cranky    5\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "url = 'https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-05-05/villagers.csv'\n",
    "villagers = pd.read_csv(url)\n",
    "\n",
    "# Group by 'species' and describe the 'personality' column\n",
    "grouped_description = villagers.groupby(\"species\")[\"personality\"].describe()\n",
    "\n",
    "print(grouped_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d236be",
   "metadata": {},
   "source": [
    "**Explanation of Results:**\n",
    "\n",
    "**count**: The number of villagers of each species.\n",
    "\n",
    "**unique**: The number of unique personalities within each species.\n",
    "\n",
    "**top**: The most common personality for that species.\n",
    "\n",
    "**freq**: The frequency of the most common personality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b69709a",
   "metadata": {},
   "source": [
    "***Question 8.2***\n",
    "\n",
    "1)\n",
    "For df.describe(), this function will count the non-missing vlaues in each column, so that the count might be different for each column by the number of missing values.\n",
    "2)\n",
    "For df.groupby(\"col1\")[\"col2\"].describe(), it will count the non-missing values of col2 exist within each group of col1. The count only relates the subset of data that belongs to each group of col1.\n",
    "\n",
    "Therefore, df.groupby(\"col1\")[\"col2\"].describe() will count the non-missing values in col2 for each group (defined by col1), while df.describe() counts the non-missing values for col2 in the enitre dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c44bb4",
   "metadata": {},
   "source": [
    "***Question 8.3***\n",
    "\n",
    "**A: Forget to include import pandas as pd in the code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "adfc4858",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mread_csv(url)\n\u001b[1;32m      3\u001b[0m df\u001b[38;5;241m.\u001b[39misna()\u001b[38;5;241m.\u001b[39msum()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "url = \"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\"\n",
    "df = pd.read_csv(url)\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23198a7",
   "metadata": {},
   "source": [
    "*In this problem, both ChatGPT and Google effectively point out to import the pandas library at the beginning of the code.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487fb043",
   "metadata": {},
   "source": [
    "**B: Mistype \"titanic.csv\" as \"titanics.csv\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ad15da5",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'titanics.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Load the Titanic dataset\u001b[39;00m\n\u001b[1;32m      4\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtitanics.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 5\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m df\u001b[38;5;241m.\u001b[39misna()\u001b[38;5;241m.\u001b[39msum()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/parsers/readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    945\u001b[0m )\n\u001b[1;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/parsers/readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    608\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    610\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 611\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1448\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1445\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1447\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1448\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1705\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1703\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1704\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1705\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1706\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1707\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1708\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1709\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1710\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1711\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1712\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1713\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1714\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1715\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1716\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/common.py:863\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    858\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    859\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    860\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    861\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    862\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 863\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    864\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    866\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    867\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    868\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    869\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    871\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    872\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'titanics.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the Titanic dataset\n",
    "url = \"titanics.csv\"\n",
    "df = pd.read_csv(url)\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8652b7",
   "metadata": {},
   "source": [
    "*In this problem, ChatGPT correctly points out that the file name should be titanic.csv and not titanics.csv, and suggests me to check the url. However, Google search doesn't show the exactly same problem, but points out that it may has the misspelled file name.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b254c542",
   "metadata": {},
   "source": [
    "**C: Try to use a dataframe before it's been assigned into the variable**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6bb61be3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DF' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      5\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(url)\n\u001b[0;32m----> 6\u001b[0m \u001b[43mDF\u001b[49m\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcol1\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcol2\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mdescribe()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'DF' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the Titanic dataset\n",
    "url = \"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\"\n",
    "df = pd.read_csv(url)\n",
    "DF.groupby(\"col1\")[\"col2\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f17db84",
   "metadata": {},
   "source": [
    "*In this problem, ChatGPT finds the problem and tells me that Python is case-sensitive. It also analyzes that I want to declare the DataFrame as df, so suggests me change the DF to df. However, in Google search, it doesn't mention the problem with letter case.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "addec89d",
   "metadata": {},
   "source": [
    "**D: Forget one of the parentheses somewhere the code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4aad3729",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (1643999381.py, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[6], line 5\u001b[0;36m\u001b[0m\n\u001b[0;31m    df = pd.read_csv(url\u001b[0m\n\u001b[0m                        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the Titanic dataset\n",
    "url = \"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\"\n",
    "df = pd.read_csv(url"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f1174e",
   "metadata": {},
   "source": [
    "*In this problem, ChatGPT finds that the line df = pd.read_csv(url is incomplete, missing the closing parenthesis ). A similar solution for missing closing parenthesis was found in a Google search, but the search results were further down the list and it took a while to find it.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ffd5ad",
   "metadata": {},
   "source": [
    "**E: Mistype one of the names of the chained functions with the code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7abe4d58",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'group_by'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_118/2009313672.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Load the Titanic dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup_by\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"col1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"col2\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescribe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   6200\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accessors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6201\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6202\u001b[0m         ):\n\u001b[1;32m   6203\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6204\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'group_by'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the Titanic dataset\n",
    "url = \"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\"\n",
    "df = pd.read_csv(url)\n",
    "df.group_by(\"col1\")[\"col2\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c6d77b",
   "metadata": {},
   "source": [
    "*In this case, both ChatGPT and Google search points out to replace group_by with groupby in the code effectively.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8daef081",
   "metadata": {},
   "source": [
    "**F: Use a column name that's not in your data for the groupby and column selection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "946b4b49",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'sex'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      5\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(url)\n\u001b[0;32m----> 6\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msex\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mage\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mdescribe()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/frame.py:8869\u001b[0m, in \u001b[0;36mDataFrame.groupby\u001b[0;34m(self, by, axis, level, as_index, sort, group_keys, observed, dropna)\u001b[0m\n\u001b[1;32m   8866\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m level \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m by \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   8867\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou have to supply one of \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mby\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlevel\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 8869\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameGroupBy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   8870\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8871\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8872\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8873\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8874\u001b[0m \u001b[43m    \u001b[49m\u001b[43mas_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mas_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8875\u001b[0m \u001b[43m    \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8876\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroup_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8877\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobserved\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobserved\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8878\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdropna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8879\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/groupby/groupby.py:1278\u001b[0m, in \u001b[0;36mGroupBy.__init__\u001b[0;34m(self, obj, keys, axis, level, grouper, exclusions, selection, as_index, sort, group_keys, observed, dropna)\u001b[0m\n\u001b[1;32m   1275\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropna \u001b[38;5;241m=\u001b[39m dropna\n\u001b[1;32m   1277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m grouper \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1278\u001b[0m     grouper, exclusions, obj \u001b[38;5;241m=\u001b[39m \u001b[43mget_grouper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1279\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1280\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1281\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1282\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1283\u001b[0m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1284\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobserved\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobserved\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mno_default\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobserved\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1285\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdropna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1286\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1288\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m observed \u001b[38;5;129;01mis\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mno_default:\n\u001b[1;32m   1289\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(ping\u001b[38;5;241m.\u001b[39m_passed_categorical \u001b[38;5;28;01mfor\u001b[39;00m ping \u001b[38;5;129;01min\u001b[39;00m grouper\u001b[38;5;241m.\u001b[39mgroupings):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/groupby/grouper.py:1009\u001b[0m, in \u001b[0;36mget_grouper\u001b[0;34m(obj, key, axis, level, sort, observed, validate, dropna)\u001b[0m\n\u001b[1;32m   1007\u001b[0m         in_axis, level, gpr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, gpr, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1008\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1009\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(gpr)\n\u001b[1;32m   1010\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(gpr, Grouper) \u001b[38;5;129;01mand\u001b[39;00m gpr\u001b[38;5;241m.\u001b[39mkey \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1011\u001b[0m     \u001b[38;5;66;03m# Add key to exclusions\u001b[39;00m\n\u001b[1;32m   1012\u001b[0m     exclusions\u001b[38;5;241m.\u001b[39madd(gpr\u001b[38;5;241m.\u001b[39mkey)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'sex'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the Titanic dataset\n",
    "url = \"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\"\n",
    "df = pd.read_csv(url)\n",
    "df.groupby(\"sex\")[\"age\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65be2742",
   "metadata": {},
   "source": [
    "*In this problem, ChatGPT explains The KeyError: 'sex' means that the column name \"sex\" is not found in the DataFrame. It also tells me this is likely because the column names in the dataset are case-sensitive, and in the Titanic dataset, the column name is \"Sex\" with an uppercase \"S\", and \"Age\" with an uppercase \"A\". However, in Google search, there are only mentions of possible word case issues when encountering KeyError, but there are no examples of the exact same situation as mine.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc365363",
   "metadata": {},
   "source": [
    "**G: Forget to put the column name as a string in quotes for the groupby and column selection, and see if the ChatBot and google are still as helpful as they were for the previous question**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "927c5715",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Sex' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      5\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(url)\n\u001b[0;32m----> 6\u001b[0m df\u001b[38;5;241m.\u001b[39mgroupby(\u001b[43mSex\u001b[49m)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAge\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mdescribe()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Sex' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the Titanic dataset\n",
    "url = \"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\"\n",
    "df = pd.read_csv(url)\n",
    "df.groupby(Sex)[\"Age\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f44b216",
   "metadata": {},
   "source": [
    "*In that case, ChatGPT finds The error NameError: name 'Sex' is not defined occurs because \"Sex\" needs to be enclosed in quotes as it's a string (the name of a column), not a variable. However, I didn't find the solution for this error in Google Search.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf760d7",
   "metadata": {},
   "source": [
    "***Conclusion of Question 8.3***\n",
    "\n",
    "After solving these problems, I don't think a google search for the error provides the necessary toubleshooting help more quickly than ChatGPT. In many problems, ChatGPT could have told me, in response to my code's reporting of the error, that I should be looking at the In many cases, ChatGPT can tell me what to change in my code for the reported error, whereas a google search more often than not just says what the cause of the error is and how to fix it, and it takes a lot longer to find a solution for your own code errors. In contrast, ChatGPT can point out errors in code and provide correct solutions in a much shorter period of time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4181c75f",
   "metadata": {},
   "source": [
    "***Question 9***\n",
    "\n",
    "**Have you reviewed the course wiki-textbook and interacted with a ChatBot (or, if that wasn't sufficient, real people in the course piazza discussion board or TA office hours) to help you understand all the material in the tutorial and lecture that you didn't quite follow when you first saw it?**\n",
    "\n",
    "Yes, I have."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd872f2",
   "metadata": {},
   "source": [
    "### Summary of Interactions Part 1:\n",
    "\n",
    "https://chatgpt.com/share/66e3a2da-e284-8013-84f8-e77b8ec3adfd\n",
    "\n",
    "1. **Initial Dataset Exploration**:\n",
    "   - You provided the link to a Titanic dataset and asked to analyze it.\n",
    "   - I guided you on how to load the dataset using pandas (`pd.read_csv()`) and inspect the **columns** and **shape** of the dataset using `df.columns` and `df.shape`.\n",
    "\n",
    "2. **Explanation of Observations and Variables**:\n",
    "   - **Observations**: Represent the rows in the dataset, where each row corresponds to a passenger on the Titanic.\n",
    "   - **Variables**: Represent the columns in the dataset, where each column captures an attribute (e.g., age, fare, survival status) of the passengers.\n",
    "\n",
    "3. **Summary Statistics**:\n",
    "   - I explained how you could use `df.describe()` to get summary statistics (mean, standard deviation, quartiles, etc.) for the numerical columns.\n",
    "   - I also showed how `df['column'].value_counts()` can give frequency counts for categorical variables, such as `Survived` and `Pclass`.\n",
    "\n",
    "4. **Discrepancies in `df.shape` and `df.describe()`**:\n",
    "   - **Number of columns analyzed**: `df.describe()` only analyzes numerical columns by default, whereas `df.shape` includes all columns (both numerical and non-numerical).\n",
    "   - **Count values in `df.describe()`**: The \"count\" in `df.describe()` represents non-missing values, which might be lower than the total number of rows in the dataset due to missing (NaN) values.\n",
    "\n",
    "5. **Difference Between Attributes and Methods**:\n",
    "   - **Attributes** (e.g., `df.shape`): Static properties of a DataFrame that describe its state (e.g., the number of rows and columns). They do not require parentheses when accessed.\n",
    "   - **Methods** (e.g., `df.describe()`): Functions associated with the DataFrame that perform actions or computations. They require parentheses to execute.\n",
    "\n",
    "6. **Detailed Explanation of Summary Statistics from `df.describe()`**:\n",
    "   - For each variable analyzed by `df.describe()`, I explained the meaning of the following summary statistics: **count**, **mean**, **standard deviation (std)**, **min**, **25% (first quartile)**, **50% (median)**, **75% (third quartile)**, and **max**.\n",
    "   - I provided an example to illustrate how to interpret these summary statistics for columns such as `Age` and `Fare`.\n",
    "   \n",
    "   \n",
    "### Summary of Interactions Part 2:\n",
    "\n",
    "https://chatgpt.com/share/66e3a307-7054-8013-ad54-a9b56ff55841\n",
    "\n",
    "### 1. **Using `df.dropna()` vs `del df['col']`:**\n",
    "\n",
    "We discussed scenarios where one method might be preferred over the other:\n",
    "\n",
    "- **Using `df.dropna()`**:\n",
    "  - When you want to remove rows with missing values while retaining other columns.\n",
    "  - Example: Cleaning rows that contain `NaN` in any column, without deleting entire columns.\n",
    "\n",
    "- **Using `del df['col']`**:\n",
    "  - When you want to remove an entire column, especially if it contains irrelevant data or too many `NaN` values.\n",
    "  - Example: Removing an entire column that is unnecessary or fully composed of `NaN` values.\n",
    "\n",
    "### 2. **Order of Operations: `del df['col']` before `df.dropna()`**:\n",
    "\n",
    "We explored why applying `del df['col']` first can be important:\n",
    "- **Efficiency**: Removing irrelevant columns first can make the subsequent `df.dropna()` faster by reducing the data size.\n",
    "- **Avoid Unnecessary Row Deletions**: Dropping irrelevant columns prevents rows with missing values in those columns from being deleted if the rest of the data is valid.\n",
    "- **Data Integrity**: Ensures that row deletions are based only on the columns relevant to the analysis.\n",
    "\n",
    "An example was provided where a column with all `NaN` values was removed before using `df.dropna()` to avoid unintended row deletions.\n",
    "\n",
    "\n",
    "### Summary of Interactions Part 3:\n",
    "\n",
    "https://chatgpt.com/share/66e3a314-c188-8013-a0bc-bf4563eff75c\n",
    "\n",
    "**1. Initial Question:**\n",
    "- You asked for help understanding the code `df.groupby(\"col1\")[\"col2\"].describe()` and requested an example using the \"Animal Crossings\" dataset.\n",
    "- I explained that this code groups a DataFrame by `col1` and then provides summary statistics for `col2` within each group using `.describe()`. I provided a step-by-step breakdown and an example using the `species` and `personality` columns.\n",
    "\n",
    "**2. Missing Values and `df.describe()`:**\n",
    "- You asked why `df.describe()` produces different counts than `df.groupby(\"col1\")[\"col2\"].describe()` when there are missing values.\n",
    "- I explained that `df.describe()` counts non-missing values for each column independently, while `df.groupby(\"col1\")[\"col2\"].describe()` provides counts of non-missing values of `col2` within each group of `col1`, which can differ based on how missing data is distributed within groups.\n",
    "\n",
    "**3. Code Errors and Debugging:**\n",
    "   - **NameError: name 'pd' is not defined:**\n",
    "     - You encountered an error related to `pd` not being defined. I explained that you need to import `pandas` before using it and provided a fix.\n",
    "   \n",
    "   - **FileNotFoundError: No such file or directory: 'titanics.csv':**\n",
    "     - You faced a `FileNotFoundError` while loading a file. I suggested verifying the file name (`titanics.csv` should be `titanic.csv`) or providing the correct file path or URL.\n",
    "   \n",
    "   - **NameError: name 'DF' is not defined:**\n",
    "     - You encountered a `NameError` because of incorrect capitalization (`DF` instead of `df`). I explained that Python is case-sensitive and recommended using consistent capitalization.\n",
    "   \n",
    "   - **SyntaxError: incomplete input:**\n",
    "     - You had a syntax error because a function call was incomplete. I pointed out that the issue was a missing closing parenthesis and provided a corrected version of the code.\n",
    "   \n",
    "   - **AttributeError: 'DataFrame' object has no attribute 'group_by':**\n",
    "     - I clarified that `groupby` is the correct method, not `group_by`, and suggested using the correct syntax.\n",
    "   \n",
    "   - **KeyError: 'sex':**\n",
    "     - You faced a `KeyError` due to using the wrong case (`sex` instead of `Sex`). I advised using the correct column name with proper case-sensitivity.\n",
    "   \n",
    "   - **NameError: name 'Sex' is not defined:**\n",
    "     - You encountered a `NameError` because `\"Sex\"` was not enclosed in quotes. I explained that column names need to be enclosed in quotes since they are strings."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
